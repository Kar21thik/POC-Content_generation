{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68330e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from typing import Dict, Any\n",
    "\n",
    "# --- New Imports for Modern Agent and Tools ---\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.tools import Tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from dotenv import load_dotenv\n",
    "import textstat # Library for readability scores\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# ==============================================================================\n",
    "#  TOOL LOGIC FUNCTIONS (Separated for clarity)\n",
    "# ==============================================================================\n",
    "\n",
    "def check_length_and_structure(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyzes content length and structure (word count, paragraphs).\n",
    "    This is fast and rule-based, perfect for a custom tool.\n",
    "    Returns a JSON string with the analysis.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    word_count = len(words)\n",
    "    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    issues = []\n",
    "\n",
    "    if word_count < 15:\n",
    "        issues.append(f\"Content is very short with only {word_count} words.\")\n",
    "    if word_count > 150 and len(paragraphs) == 1:\n",
    "        issues.append(\"Long content is not broken into paragraphs, making it hard to read.\")\n",
    "    if len(sentences) < 2 and word_count > 20:\n",
    "        issues.append(\"Content consists of a single long sentence; consider breaking it up.\")\n",
    "\n",
    "    return json.dumps({\"word_count\": word_count, \"sentences\": len(sentences), \"paragraphs\": len(paragraphs), \"issues\": issues})\n",
    "\n",
    "def check_grammar_with_llm(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses a powerful LLM to check for grammar and spelling errors.\n",
    "    This is more accurate than a fixed list of regex patterns.\n",
    "    Returns a JSON string listing the errors found.\n",
    "    \"\"\"\n",
    "    print(\"ðŸ§  Using LLM to check grammar...\")\n",
    "    checker_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert proofreader. Analyze the following text for grammar and spelling mistakes.\n",
    "    Respond with a JSON object containing one key, \"errors\", which is a list of strings.\n",
    "    Each string should describe a specific error you found. If no errors, return an empty list.\n",
    "    Text: \"{text}\"\n",
    "    \"\"\"\n",
    "    response = checker_llm.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "def check_readability_with_textstat(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses the 'textstat' library to calculate objective readability scores.\n",
    "    This is a custom tool enhanced with a specialized library.\n",
    "    Returns a JSON string with the Flesch Reading Ease score and interpretation.\n",
    "    \"\"\"\n",
    "    if not text.strip():\n",
    "        return json.dumps({\"score\": 0, \"level\": \"unassessable\"})\n",
    "    \n",
    "    score = textstat.flesch_reading_ease(text)\n",
    "    level = \"Very Easy\"\n",
    "    if score < 30:\n",
    "        level = \"Very Confusing (College Graduate)\"\n",
    "    elif score < 60:\n",
    "        level = \"Difficult\"\n",
    "    elif score < 80:\n",
    "        level = \"Fairly Easy\"\n",
    "    return json.dumps({\"flesch_reading_ease_score\": score, \"level\": level})\n",
    "\n",
    "def check_professionalism(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Checks for unprofessional content like profanity, slang, or excessive formatting.\n",
    "    This is fast and rule-based, perfect for a custom tool.\n",
    "    Returns a JSON string with a list of professionalism issues.\n",
    "    \"\"\"\n",
    "    issues = []\n",
    "    # Check for profanity\n",
    "    if re.search(r'\\b(fuck|shit|damn|bitch|asshole)\\b', text, re.IGNORECASE):\n",
    "        issues.append(\"Inappropriate or profane language was found.\")\n",
    "    # Check for \"shouting\" (excessive capitalization)\n",
    "    if re.search(r'\\b[A-Z]{4,}\\b', text) and sum(1 for c in text if c.isupper()) / len(text) > 0.3:\n",
    "        issues.append(\"Excessive capitalization is used, which appears unprofessional.\")\n",
    "    # Check for excessive punctuation\n",
    "    if re.search(r'[!?@#$%^&*()]{4,}', text):\n",
    "        issues.append(\"Excessive punctuation or symbols are used.\")\n",
    "    return json.dumps({\"issues\": issues})\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "#  THE REFACTORED AGENT CLASS\n",
    "# ==============================================================================\n",
    "\n",
    "class ContentQualityAgent:\n",
    "    def __init__(self, model=\"gpt-4o-mini\", temperature=0):\n",
    "        self.llm = ChatOpenAI(model=model, temperature=temperature)\n",
    "        self.setup_agent()\n",
    "\n",
    "    def setup_agent(self):\n",
    "        \"\"\"Setup an agent with a mix of built-in and custom tools.\"\"\"\n",
    "        \n",
    "        # 1. Define the tools list\n",
    "        tools = [\n",
    "            # --- BUILT-IN TOOL ---\n",
    "            TavilySearchResults(\n",
    "                name=\"FactCheckSearch\",\n",
    "                max_results=3,\n",
    "                description=\"Use this tool to search the web to verify factual claims, check for misinformation, or find context on a topic.\"\n",
    "            ),\n",
    "            # --- UPGRADED & CUSTOM TOOLS ---\n",
    "            Tool(\n",
    "                name=\"GrammarAndSpellingCheck\",\n",
    "                func=check_grammar_with_llm,\n",
    "                description=\"Use this to check a piece of text for spelling and grammatical errors.\"\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"ReadabilityCheck\",\n",
    "                func=check_readability_with_textstat,\n",
    "                description=\"Use this to get the readability score and complexity level of a text.\"\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"ProfessionalismCheck\",\n",
    "                func=check_professionalism,\n",
    "                description=\"Use this to check for unprofessional language, like profanity, excessive capitalization, or slang.\"\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"LengthAndStructureCheck\",\n",
    "                func=check_length_and_structure,\n",
    "                description=\"Use this to get basic statistics like word count and paragraph structure.\"\n",
    "            ),\n",
    "        ]\n",
    "        \n",
    "        # 2. Create the agent's prompt template\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are an expert Quality Assurance assistant. Your goal is to provide a complete and accurate quality report on the user's text. Use your tools to check all aspects of the content. First, form a plan, then execute your plan by calling the necessary tools. Finally, compile all the results into a comprehensive final answer summarizing your findings.\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "            (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "        ])\n",
    "        \n",
    "        # 3. Create the agent\n",
    "        agent = create_openai_functions_agent(self.llm, tools, prompt)\n",
    "        \n",
    "        # 4. Create the Agent Executor to run the agent\n",
    "        self.agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "    def validate(self, text_to_validate: str) -> Dict[str, Any]:\n",
    "        \"\"\"Runs the agent to perform a full quality analysis.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Please provide a comprehensive quality analysis of the following text.\n",
    "        Your final answer should be a summary of all findings.\n",
    "\n",
    "        Text to Analyze:\n",
    "        ---\n",
    "        {text_to_validate}\n",
    "        ---\n",
    "        \"\"\"\n",
    "        return self.agent_executor.invoke({\"input\": prompt})\n",
    "\n",
    "# ==============================================================================\n",
    "#  MAIN EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "\n",
    "def main():\n",
    "    if not os.getenv(\"OPENAI_API_KEY\") or not os.getenv(\"TAVILY_API_KEY\"):\n",
    "        print(\"âŒ Error: OPENAI_API_KEY and TAVILY_API_KEY must be set in your .env file.\")\n",
    "        return\n",
    "\n",
    "    print(\"ðŸš€ Initializing Content Quality Agent...\")\n",
    "    validator = ContentQualityAgent()\n",
    "\n",
    "    sample_text = \"\"\"\n",
    "    BREAKING NEWS!!! The goverment has SECRETLEY been using 5G towers to controll our minds and make us buy more STUFF we dont need!!! This is CIENTIFICALY proven by my frend who works at Area 51 and he told me that aliens are actualy running Facebook and Google to steal our THOUGHTS!!! Mark Zuckerberg is definatley an alien REPTILIAN who eats babys for breakfast FACT!!! Wake up SHEEPLE the truth is RIGHT THERE!!!! #conspiracy #truth #wakeup !@#$%^&*()\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ” Running Full Analysis...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    result = validator.validate(sample_text)\n",
    "    \n",
    "    print(\"\\nâœ… FINAL AGENT REPORT:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(result['output'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bb63d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from typing import Dict, Any, Union\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.tools import Tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from dotenv import load_dotenv\n",
    "import textstat\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ==============================================================================\n",
    "#  TOOL LOGIC FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def check_length_and_structure(text: str) -> str:\n",
    "    \"\"\"Analyzes content length and structure (word count, paragraphs).\"\"\"\n",
    "    # This function is now focused on a single piece of text\n",
    "    words = text.split()\n",
    "    word_count = len(words)\n",
    "    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    issues = []\n",
    "    if word_count < 15:\n",
    "        issues.append(f\"Content is very short with only {word_count} words.\")\n",
    "    if word_count > 150 and len(paragraphs) == 1:\n",
    "        issues.append(\"Long content is not broken into paragraphs, making it hard to read.\")\n",
    "    if len(sentences) < 2 and word_count > 20:\n",
    "        issues.append(\"Content consists of a single long sentence; consider breaking it up.\")\n",
    "    return json.dumps({\"word_count\": word_count, \"sentences\": len(sentences), \"paragraphs\": len(paragraphs), \"issues\": issues})\n",
    "\n",
    "def check_grammar_with_llm(text: str) -> str:\n",
    "    \"\"\"Uses a powerful LLM to check for grammar and spelling errors.\"\"\"\n",
    "    print(\"ðŸ§  Using LLM to check grammar...\")\n",
    "    checker_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert proofreader. Analyze the following text for grammar and spelling mistakes.\n",
    "    Respond with a JSON object: {{\"errors\": [\"list of concise error descriptions\"]}}. If no errors, return an empty list.\n",
    "    Text: \"{text}\"\n",
    "    \"\"\"\n",
    "    response = checker_llm.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "def check_readability_with_textstat(text: str) -> str:\n",
    "    \"\"\"Uses the 'textstat' library to calculate objective readability scores.\"\"\"\n",
    "    if not text.strip():\n",
    "        return json.dumps({\"score\": 0, \"level\": \"unassessable\"})\n",
    "    score = textstat.flesch_reading_ease(text)\n",
    "    level = \"Very Easy\"\n",
    "    if score < 30: level = \"Very Confusing (College Graduate)\"\n",
    "    elif score < 60: level = \"Difficult\"\n",
    "    elif score < 80: level = \"Fairly Easy\"\n",
    "    return json.dumps({\"flesch_reading_ease_score\": score, \"level\": level})\n",
    "\n",
    "def check_professionalism(text: str) -> str:\n",
    "    \"\"\"Checks for unprofessional content like profanity, slang, or excessive formatting.\"\"\"\n",
    "    issues = []\n",
    "    if re.search(r'\\b(fuck|shit|damn|bitch|asshole)\\b', text, re.IGNORECASE):\n",
    "        issues.append(\"Inappropriate or profane language was found.\")\n",
    "    if re.search(r'\\b[A-Z]{4,}\\b', text) and sum(1 for c in text if c.isupper()) / len(text) > 0.3:\n",
    "        issues.append(\"Excessive capitalization is used, which appears unprofessional.\")\n",
    "    if re.search(r'[!?@#$%^&*()]{4,}', text):\n",
    "        issues.append(\"Excessive punctuation or symbols are used.\")\n",
    "    return json.dumps({\"issues\": issues})\n",
    "\n",
    "def check_redundancy(text: str) -> str:\n",
    "    \"\"\"Analyzes the text for repetitive sentences and overused words.\"\"\"\n",
    "    sentences = [s.lower().strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n",
    "    issues = []\n",
    "    if len(sentences) > 2:\n",
    "        redundancy_ratio = len(set(sentences)) / len(sentences)\n",
    "        if redundancy_ratio < 0.5:\n",
    "            issues.append(f\"High sentence redundancy detected. {100 - redundancy_ratio*100:.0f}% of sentences are repetitive.\")\n",
    "    return json.dumps({\"redundancy_issues\": issues})\n",
    "\n",
    "def check_itext_payload_faithfulness(payload_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyzes an iText JSON payload. It checks for consistency between the message and data,\n",
    "    and evaluates the faithfulness and relevance between the 'content', 'simplified', and 'elaborated' versions.\n",
    "    \"\"\"\n",
    "    print(\"ðŸ§  Using LLM to analyze iText payload...\")\n",
    "    try:\n",
    "        payload = json.loads(payload_str)\n",
    "        message = payload.get(\"message\", \"\")\n",
    "        content_data = payload.get(\"data\", [{}])[0].get(\"data\", {})\n",
    "        original = content_data.get(\"content\", \"\")\n",
    "        simplified = content_data.get(\"simplified\", \"\")\n",
    "        elaborated = content_data.get(\"elaborated\", \"\")\n",
    "    except (json.JSONDecodeError, IndexError, KeyError) as e:\n",
    "        return json.dumps({\"error\": f\"Invalid iText payload structure: {e}\"})\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert iText payload evaluator. Analyze the following payload:\n",
    "    Message: \"{message}\"\n",
    "    Original Content: \"{original}\"\n",
    "    Simplified Content: \"{simplified}\"\n",
    "    Elaborated Content: \"{elaborated}\"\n",
    "\n",
    "    Evaluate two things:\n",
    "    1.  **Consistency**: Does the content seem to match the message (e.g., a \"success\" message with valid content)?\n",
    "    2.  **Faithfulness & Relevance**: Compare the three text versions. Do the simplified/elaborated versions maintain the original meaning without adding incorrect info?\n",
    "\n",
    "    Return ONLY a JSON object with your findings, like this:\n",
    "    {{\n",
    "        \"consistency_check\": \"<Brief summary of consistency>\",\n",
    "        \"faithfulness_score\": <int 1-10>,\n",
    "        \"relevance_score\": <int 1-10>,\n",
    "        \"comparison_summary\": \"<Brief summary of how the versions differ>\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "# ==============================================================================\n",
    "#  AGENT CLASS\n",
    "# ==============================================================================\n",
    "\n",
    "class ContentQualityAgent:\n",
    "    def __init__(self, model=\"gpt-4o-mini\", temperature=0):\n",
    "        self.llm = ChatOpenAI(model=model, temperature=temperature)\n",
    "        self.setup_agent()\n",
    "\n",
    "    def setup_agent(self):\n",
    "        tools = [\n",
    "            Tool(name=\"iTextPayloadFaithfulnessCheck\", func=check_itext_payload_faithfulness, description=\"Use this ONLY for a JSON input representing an iText payload. It checks faithfulness between content versions.\"),\n",
    "            Tool(name=\"GrammarAndSpellingCheck\", func=check_grammar_with_llm, description=\"Use to check a piece of text for spelling and grammatical errors.\"),\n",
    "            Tool(name=\"ReadabilityCheck\", func=check_readability_with_textstat, description=\"Use to get the readability score of a piece of text.\"),\n",
    "            Tool(name=\"ProfessionalismCheck\", func=check_professionalism, description=\"Use to check a piece of text for unprofessional language.\"),\n",
    "            Tool(name=\"LengthAndStructureCheck\", func=check_length_and_structure, description=\"Use to get word count and structure of a piece of text.\"),\n",
    "            Tool(name=\"RedundancyCheck\", func=check_redundancy, description=\"Use to check a piece of text for repetitive sentences.\"),\n",
    "            TavilySearchResults(name=\"FactCheckSearch\", max_results=3, description=\"Use to verify factual claims in a piece of text.\")\n",
    "        ]\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are an expert Quality Assurance assistant. Your goal is to provide a complete quality report.\n",
    "\n",
    "            You will receive one of two types of input:\n",
    "            1.  A plain text paragraph.\n",
    "            2.  A JSON string representing an \"iText payload\".\n",
    "\n",
    "            **Your Plan:**\n",
    "            - If the input is an iText payload (JSON), you MUST use the `iTextPayloadFaithfulnessCheck` tool on the entire JSON. You should ALSO run standard checks (grammar, professionalism, etc.) on the 'content' field within the payload.\n",
    "            - If the input is plain text, just run the standard checks on it.\n",
    "            \n",
    "            After executing your plan, compile all results into the final JSON report.\n",
    "\n",
    "            **FINAL ANSWER FORMATTING INSTRUCTIONS:**\n",
    "            Your final answer MUST be a single JSON object with the specified structure.\n",
    "            {{\n",
    "                \"overall_score\": <An integer score from 1-10>,\n",
    "                \"category_scores\": {{\n",
    "                    \"grammar_and_spelling\": <Score 1-10>,\n",
    "                    \"readability\": <Score 1-10>,\n",
    "                    \"professionalism_and_tone\": <Score 1-10>,\n",
    "                    \"factual_accuracy\": <Score 1-10. Default to 10 if no claims to check.>,\n",
    "                    \"redundancy\": <Score 1-10>,\n",
    "                    \"faithfulness\": <Score 1-10. Only applies to iText. Default to 10 for plain text.>\n",
    "                }},\n",
    "                \"summary\": \"<A natural language summary of key issues and suggestions for improvement.>\"\n",
    "            }}\n",
    "            \"\"\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "            (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "        ])\n",
    "        \n",
    "        agent = create_openai_functions_agent(self.llm, tools, prompt)\n",
    "        self.agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "    async def validate_async(self, input_data: Union[str, Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Runs the agent asynchronously on either a string or a dictionary.\"\"\"\n",
    "        \n",
    "        if isinstance(input_data, dict):\n",
    "            # It's an iText payload, serialize it to a string for the agent\n",
    "            input_str = json.dumps(input_data, indent=2)\n",
    "            analysis_subject = \"the following iText payload\"\n",
    "        else:\n",
    "            # It's a plain paragraph\n",
    "            input_str = input_data\n",
    "            analysis_subject = \"the following text\"\n",
    "            \n",
    "        prompt = f\"Please provide a comprehensive quality analysis of {analysis_subject}:\\n---\\n{input_str}\\n---\"\n",
    "        \n",
    "        result = {} \n",
    "        try:\n",
    "            result = await self.agent_executor.ainvoke({\"input\": prompt})\n",
    "            # Use regex to find the JSON object in case the LLM adds extra text\n",
    "            json_match = re.search(r'\\{.*\\}', result['output'], re.DOTALL)\n",
    "            if json_match:\n",
    "                return json.loads(json_match.group(0))\n",
    "            else:\n",
    "                raise json.JSONDecodeError(\"No JSON object found in agent output.\", result['output'], 0)\n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            print(f\"âŒ Error during agent execution or parsing: {e}\")\n",
    "            return {\"error\": \"Failed to generate a valid report.\", \"raw_output\": result.get('output', 'No output was generated.')}\n",
    "\n",
    "# ==============================================================================\n",
    "#  MAIN EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "async def main():\n",
    "    if not os.getenv(\"OPENAI_API_KEY\") or not os.getenv(\"TAVILY_API_KEY\"):\n",
    "        print(\"âŒ Error: API keys must be set in your .env file.\")\n",
    "        return\n",
    "\n",
    "    print(\"ðŸš€ Initializing Content Quality Agent...\")\n",
    "    validator = ContentQualityAgent()\n",
    "    \n",
    "    # # --- CASE 1: Plain Paragraph ---\n",
    "    # paragraph_to_analyze = \"\"\"\n",
    "    # Machine learning is the most revolutionary technology ever created and will solve all human problems. However, machine learning is completely useless and will never work properly. AI systems are incredibly intelligent. AI is actually very stupid. Neural networks are perfect. Neural networks are terrible.\n",
    "    # \"\"\"\n",
    "    # print(\"\\n\\nðŸ” Running analysis on Plain Paragraph...\")\n",
    "    # print(\"=\" * 60)\n",
    "    # report1 = await validator.validate_async(paragraph_to_analyze)\n",
    "    # print_report(report1)\n",
    "\n",
    "    # --- CASE 2: iText Payload ---\n",
    "    itext_payload = {\n",
    "        \"message\": \"Successfully generated iText block data.\",\n",
    "        \"data\": [\n",
    "            {\n",
    "                \"type\": \"iText\",\n",
    "                \"data\": {\n",
    "                    \"content\": \"The sun is a star at the center of the Solar System. It is a nearly perfect ball of hot plasma. The Sun's diameter is about 109 times that of Earth.\",\n",
    "                    \"simplified\": \"The sun is a big star in the middle of our solar system.\",\n",
    "                    \"elaborated\": \"The Sun, a G-type main-sequence star (G2V), is the focal point of our Solar System. Comprising over 99.8% of the system's mass, it is an immense sphere of incandescent plasma, with a diameter of approximately 1.39 million kilometers, roughly 109 times that of our planet, which is a big lie.\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    print(\"\\n\\nðŸ” Running analysis on iText Payload...\")\n",
    "    print(\"=\" * 60)\n",
    "    report2 = await validator.validate_async(itext_payload)\n",
    "    print_report(report2)\n",
    "\n",
    "\n",
    "def print_report(report: Dict[str, Any]):\n",
    "    \"\"\"Helper function to neatly print the agent's report.\"\"\"\n",
    "    print(\"\\nâœ… FINAL AGENT REPORT:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    if \"error\" in report:\n",
    "        print(f\"An error occurred: {report['error']}\")\n",
    "        print(f\"Raw Output: {report.get('raw_output')}\")\n",
    "    else:\n",
    "        print(f\"ðŸ† Overall Quality Score: {report.get('overall_score', 'N/A')} / 10\")\n",
    "        print(\"\\n--- Category Breakdown ---\")\n",
    "        for category, score in report.get('category_scores', {}).items():\n",
    "            category_name = category.replace('_', ' ').title()\n",
    "            print(f\"   - {category_name}: {score}/10\")\n",
    "        \n",
    "        print(\"\\n--- Summary & Suggestions ---\")\n",
    "        print(report.get('summary', 'No summary provided.'))\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Run the main async function\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a51f877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from typing import Dict, Any, List, Union\n",
    "import asyncio\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import textstat\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ==============================================================================\n",
    "# TOOL LOGIC FUNCTIONS (Made async for concurrent execution)\n",
    "# ==============================================================================\n",
    "\n",
    "async def check_length_and_structure(text: str) -> str:\n",
    "    words = text.split()\n",
    "    word_count = len(words)\n",
    "    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    issues = []\n",
    "    if word_count < 15:\n",
    "        issues.append(f\"Content is very short with only {word_count} words.\")\n",
    "    if word_count > 150 and len(paragraphs) == 1:\n",
    "        issues.append(\"Long content is not broken into paragraphs, making it hard to read.\")\n",
    "    if len(sentences) < 2 and word_count > 20:\n",
    "        issues.append(\"Content consists of a single long sentence; consider breaking it up.\")\n",
    "    return json.dumps({\"word_count\": word_count, \"sentences\": len(sentences), \"paragraphs\": len(paragraphs), \"issues\": issues})\n",
    "\n",
    "\n",
    "async def check_grammar_with_llm(text: str) -> str:\n",
    "    print(\"ðŸ§  Using LLM to check grammar...\")\n",
    "    # Use a cheaper/faster model for simple tasks\n",
    "    checker_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert proofreader. Analyze the following text for grammar and spelling mistakes.\n",
    "    Respond ONLY with JSON: {{\"errors\": [\"list of concise error descriptions\"]}}. If no errors, return an empty list.\n",
    "    Text: \"{text}\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = await checker_llm.ainvoke(prompt)\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"errors\": [f\"LLM call failed: {e}\"]})\n",
    "\n",
    "\n",
    "async def check_readability_with_textstat(text: str) -> str:\n",
    "    if not text.strip():\n",
    "        return json.dumps({\"score\": 0, \"level\": \"unassessable\"})\n",
    "    score = textstat.flesch_reading_ease(text)\n",
    "    level = \"Very Easy\"\n",
    "    if score < 30: level = \"Very Confusing (College Graduate)\"\n",
    "    elif score < 60: level = \"Difficult\"\n",
    "    elif score < 80: level = \"Fairly Easy\"\n",
    "    return json.dumps({\"flesch_reading_ease_score\": score, \"level\": level})\n",
    "\n",
    "\n",
    "async def check_professionalism(text: str) -> str:\n",
    "    issues = []\n",
    "    if re.search(r'\\b(fuck|shit|damn|bitch|asshole)\\b', text, re.IGNORECASE):\n",
    "        issues.append(\"Inappropriate or profane language was found.\")\n",
    "    # Adjusted regex to be more accurate for excessive caps\n",
    "    if sum(1 for c in text if c.isupper()) / len(text.replace(\" \", \"\")) > 0.4 and len(text) > 20:\n",
    "         issues.append(\"Excessive capitalization is used, which appears unprofessional.\")\n",
    "    if re.search(r'[!?@#$%^&*()_+=]{4,}', text):\n",
    "        issues.append(\"Excessive punctuation or symbols are used.\")\n",
    "    return json.dumps({\"issues\": issues})\n",
    "\n",
    "\n",
    "async def check_redundancy(text: str) -> str:\n",
    "    sentences = [s.lower().strip() for s in re.split(r'[.!?]+', text) if len(s.strip().split()) > 3]\n",
    "    issues = []\n",
    "    if len(sentences) > 2:\n",
    "        # Using a more robust check for redundancy\n",
    "        unique_sentences = set(sentences)\n",
    "        redundancy_ratio = len(unique_sentences) / len(sentences)\n",
    "        if redundancy_ratio < 0.6:\n",
    "            issues.append(f\"High sentence redundancy detected. {100 - redundancy_ratio*100:.0f}% of sentences seem repetitive.\")\n",
    "    return json.dumps({\"redundancy_issues\": issues})\n",
    "\n",
    "\n",
    "async def check_faithfulness_and_relevance(payload: Dict[str, Any]) -> str:\n",
    "    \"\"\"Compare original vs simplified and elaborated text.\"\"\"\n",
    "    print(\"ðŸ§  Using LLM to check faithfulness and relevance...\")\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    content = payload.get(\"content\", \"\")\n",
    "    simplified = payload.get(\"simplified\", \"\")\n",
    "    elaborated = payload.get(\"elaborated\", \"\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert text evaluator. Compare these three versions of content:\n",
    "    [Original] {content}\n",
    "    [Simplified] {simplified}\n",
    "    [Elaborated] {elaborated}\n",
    "\n",
    "    Evaluate on a scale of 1-10 (1=bad, 10=excellent) and provide a summary.\n",
    "    1. Faithfulness â€” Do simplified/elaborated keep the original meaning?\n",
    "    2. Relevance â€” Are they contextually correct and not adding wrong info?\n",
    "    3. Accuracy â€” Are there factual or conceptual mistakes?\n",
    "\n",
    "    Return ONLY JSON with this exact structure:\n",
    "    {{\n",
    "        \"faithfulness\": <int>,\n",
    "        \"relevance\": <int>,\n",
    "        \"accuracy\": <int>,\n",
    "        \"summary\": \"<short explanation of issues or differences>\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = await llm.ainvoke(prompt)\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        return json.dumps({\n",
    "            \"faithfulness\": 1, \"relevance\": 1, \"accuracy\": 1,\n",
    "            \"summary\": f\"LLM call failed: {e}\"\n",
    "        })\n",
    "\n",
    "# ==============================================================================\n",
    "# VALIDATOR CLASS (Replaces the Agent)\n",
    "# ==============================================================================\n",
    "\n",
    "class ContentQualityValidator:\n",
    "    def __init__(self, model=\"gpt-4o\", temperature=0.1):\n",
    "        # Use a powerful model for the final synthesis step\n",
    "        self.synthesis_llm = ChatOpenAI(model=model, temperature=temperature)\n",
    "\n",
    "    async def validate_async(self, input_data: Union[str, Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Runs all checks concurrently and synthesizes a final report.\n",
    "        \"\"\"\n",
    "        tasks = []\n",
    "        is_itext = isinstance(input_data, dict) and \"content\" in input_data\n",
    "\n",
    "        # Use the 'content' field for standard checks if available, otherwise use the whole input\n",
    "        text_to_check = input_data.get(\"content\") if is_itext else input_data\n",
    "\n",
    "        # --- 1. GATHER DATA FROM ALL CHECKS ---\n",
    "        tasks.append(check_grammar_with_llm(text_to_check))\n",
    "        tasks.append(check_readability_with_textstat(text_to_check))\n",
    "        tasks.append(check_professionalism(text_to_check))\n",
    "        tasks.append(check_redundancy(text_to_check))\n",
    "        tasks.append(check_length_and_structure(text_to_check))\n",
    "        \n",
    "        if is_itext:\n",
    "            tasks.append(check_faithfulness_and_relevance(input_data))\n",
    "\n",
    "        # Run all tasks in parallel\n",
    "        results = await asyncio.gather(*tasks)\n",
    "\n",
    "        # --- 2. PREPARE CONTEXT FOR SYNTHESIS ---\n",
    "        context = {\n",
    "            \"grammar_and_spelling_report\": json.loads(results[0]),\n",
    "            \"readability_report\": json.loads(results[1]),\n",
    "            \"professionalism_report\": json.loads(results[2]),\n",
    "            \"redundancy_report\": json.loads(results[3]),\n",
    "            \"length_and_structure_report\": json.loads(results[4]),\n",
    "        }\n",
    "        if is_itext:\n",
    "            context[\"faithfulness_and_relevance_report\"] = json.loads(results[5])\n",
    "        \n",
    "        # --- 3. SYNTHESIZE THE FINAL REPORT ---\n",
    "        print(\"ðŸ§  Synthesizing final report...\")\n",
    "        synthesis_prompt = f\"\"\"\n",
    "        You are an intelligent QA Analyst. Based on the following raw data from various quality checks,\n",
    "        generate a final JSON report.\n",
    "\n",
    "        Your task is to convert the raw data into category scores from 1-10 (1=very poor, 10=excellent)\n",
    "        and write a concise summary with actionable improvement suggestions.\n",
    "\n",
    "        Raw Data:\n",
    "        {json.dumps(context, indent=2)}\n",
    "\n",
    "        - For `grammar_and_spelling`, if there are errors, score is low. No errors = 10.\n",
    "        - For `readability`, a \"Very Easy\" or \"Fairly Easy\" level is high-scoring (8-10). \"Difficult\" is 4-6. \"Very Confusing\" is 1-3.\n",
    "        - For `professionalism_and_tone`, any issues found should result in a low score. No issues = 10.\n",
    "        - For `redundancy`, any issues found means a lower score. No issues = 10.\n",
    "        - `factual_accuracy` is 10 by default unless faithfulness/relevance checks find issues.\n",
    "        - `faithfulness` and `relevance` scores should be taken directly from the report if present, otherwise set them to 10.\n",
    "\n",
    "        FINAL ANSWER must be ONLY the JSON object with the following structure:\n",
    "        {{\n",
    "            \"overall_score\": <int 1-10>,\n",
    "            \"category_scores\": {{\n",
    "                \"grammar_and_spelling\": <int 1-10>,\n",
    "                \"readability\": <int 1-10>,\n",
    "                \"professionalism_and_tone\": <int 1-10>,\n",
    "                \"factual_accuracy\": <int 1-10>,\n",
    "                \"redundancy\": <int 1-10>,\n",
    "                \"faithfulness\": <int 1-10>,\n",
    "                \"relevance\": <int 1-10>\n",
    "            }},\n",
    "            \"summary\": \"<summary of findings and concrete improvement suggestions>\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            final_response = await self.synthesis_llm.ainvoke(synthesis_prompt)\n",
    "            # Find the JSON object in the response, in case the LLM adds extra text\n",
    "            json_match = re.search(r'\\{.*\\}', final_response.content, re.DOTALL)\n",
    "            if json_match:\n",
    "                return json.loads(json_match.group(0))\n",
    "            else:\n",
    "                raise json.JSONDecodeError(\"No JSON object found in the final response.\", final_response.content, 0)\n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            print(f\"âŒ Error parsing final JSON response: {e}\")\n",
    "            return {\"error\": \"Invalid or failed final JSON synthesis\", \"raw_output\": final_response.content}\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# MAIN\n",
    "# ==============================================================================\n",
    "async def main():\n",
    "    validator = ContentQualityValidator()\n",
    "\n",
    "    # # --- CASE 1: iText JSON payload ---\n",
    "    # itext_payload = {\n",
    "    #     \"message\": \"Successfully generated iText block data.\",\n",
    "    #     \"data\": [\n",
    "    #         {\n",
    "    #             \"type\": \"iText\",\n",
    "    #             \"data\": {\n",
    "    #                 \"content\": \"Foundations of Deep Learning serve as the bedrock for modern AI, encompassing neural networks, backpropagation, and activation functions. These principles enable machines to learn from vast datasets.\",\n",
    "    #                 \"simplified\": \"Deep learning is a type of machine learning that uses artificial neural networks to learn from data. It's the foundation of today's AI.\",\n",
    "    #                 \"elaborated\": \"The foundations of deep learning encompass a range of critical concepts, starting with the artificial neuron, the basic computational unit. These are organized into layers to form deep neural networks. The learning process is facilitated by an algorithm called backpropagation, which adjusts the network's parameters to minimize error, and non-linear activation functions like ReLU, which allow the network to model complex patterns. This is all a lie and the sky is green.\"\n",
    "    #             }\n",
    "    #         }\n",
    "    #     ]\n",
    "    # }\n",
    "\n",
    "    # print(\"\\nðŸš€ Evaluating iText payload...\")\n",
    "    # itext_data = itext_payload[\"data\"][0][\"data\"]\n",
    "    # report = await validator.validate_async(itext_data)\n",
    "    # print(json.dumps(report, indent=2))\n",
    "\n",
    "\n",
    "    # # --- CASE 2: plain paragraph \n",
    "# Run main\n",
    "if __name__ == \"__main__\":\n",
    "    await (main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8329151e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Initializing Content Quality Agent...\n",
      "============================================================\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `GrammarAndSpellingCheck` with `Identifying Market Needs\n",
      "\n",
      "Identifying market needs is a critical step in the ideation and concept development phase of launching a startup. Understanding what potential customers desire or require enables entrepreneurs to tailor their products or services effectively. This process involves conducting thorough market research, analyzing trends, and gathering insights directly from the target audience. Entrepreneurs must consider various factors, including demographics, psychographics, and behavioral patterns, to accurately identify market needs. By leveraging qualitative and quantitative research methods, such as surveys, interviews, and focus groups, startups can gain a comprehensive understanding of their market landscape. This foundational knowledge not only informs product development but also enhances marketing strategies, ensuring that the startup addresses genuine customer pain points.`\n",
      "\n",
      "\n",
      "\u001b[0mðŸ§  Using LLM to check grammar...\n",
      "\u001b[33;1m\u001b[1;3m```json\n",
      "{\"errors\": []}\n",
      "```\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `ReadabilityCheck` with `Identifying Market Needs\n",
      "\n",
      "Identifying market needs is a critical step in the ideation and concept development phase of launching a startup. Understanding what potential customers desire or require enables entrepreneurs to tailor their products or services effectively. This process involves conducting thorough market research, analyzing trends, and gathering insights directly from the target audience. Entrepreneurs must consider various factors, including demographics, psychographics, and behavioral patterns, to accurately identify market needs. By leveraging qualitative and quantitative research methods, such as surveys, interviews, and focus groups, startups can gain a comprehensive understanding of their market landscape. This foundational knowledge not only informs product development but also enhances marketing strategies, ensuring that the startup addresses genuine customer pain points.`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m{\"flesch_reading_ease_score\": 4.154770114942551, \"level\": \"Very Confusing (College Graduate)\"}\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `ProfessionalismCheck` with `Identifying Market Needs\n",
      "\n",
      "Identifying market needs is a critical step in the ideation and concept development phase of launching a startup. Understanding what potential customers desire or require enables entrepreneurs to tailor their products or services effectively. This process involves conducting thorough market research, analyzing trends, and gathering insights directly from the target audience. Entrepreneurs must consider various factors, including demographics, psychographics, and behavioral patterns, to accurately identify market needs. By leveraging qualitative and quantitative research methods, such as surveys, interviews, and focus groups, startups can gain a comprehensive understanding of their market landscape. This foundational knowledge not only informs product development but also enhances marketing strategies, ensuring that the startup addresses genuine customer pain points.`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[31;1m\u001b[1;3m{\"issues\": []}\u001b[0m\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"overall_score\": 8,\n",
      "    \"category_scores\": {\n",
      "        \"grammar_and_spelling\": 10,\n",
      "        \"readability\": 5,\n",
      "        \"professionalism_and_tone\": 10,\n",
      "        \"faithfulness\": 10\n",
      "    },\n",
      "    \"summary\": \"The text is grammatically correct and free of spelling errors, demonstrating a high level of professionalism and appropriate tone for a business context. However, the readability score indicates that the text is very complex, suitable for a college graduate level, which may not be accessible to all audiences. To improve, consider simplifying the language and sentence structure to enhance readability while maintaining the informative content.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "âœ… FINAL AGENT REPORT: Plain Paragraph Analysis\n",
      "------------------------------------------------------------\n",
      "ðŸ† Overall Quality Score: 8 / 10\n",
      "\n",
      "--- Category Breakdown ---\n",
      "   - Grammar And Spelling: 10/10\n",
      "   - Readability: 5/10\n",
      "   - Professionalism And Tone: 10/10\n",
      "   - Faithfulness: 10/10\n",
      "\n",
      "--- Summary & Suggestions ---\n",
      "The text is grammatically correct and free of spelling errors, demonstrating a high level of professionalism and appropriate tone for a business context. However, the readability score indicates that the text is very complex, suitable for a college graduate level, which may not be accessible to all audiences. To improve, consider simplifying the language and sentence structure to enhance readability while maintaining the informative content.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from typing import Dict, Any, Union\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.tools import Tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from dotenv import load_dotenv\n",
    "import textstat\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ==============================================================================\n",
    "#  TOOL LOGIC FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def check_length_and_structure(text: str) -> str:\n",
    "    \"\"\"Analyzes content length and structure (word count, paragraphs).\"\"\"\n",
    "    words = text.split()\n",
    "    word_count = len(words)\n",
    "    # ... (rest of the function is unchanged)\n",
    "    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    issues = []\n",
    "    if word_count < 15:\n",
    "        issues.append(f\"Content is very short with only {word_count} words.\")\n",
    "    if word_count > 150 and len(paragraphs) == 1:\n",
    "        issues.append(\"Long content is not broken into paragraphs, making it hard to read.\")\n",
    "    if len(sentences) < 2 and word_count > 20:\n",
    "        issues.append(\"Content consists of a single long sentence; consider breaking it up.\")\n",
    "    return json.dumps({\"word_count\": word_count, \"sentences\": len(sentences), \"paragraphs\": len(paragraphs), \"issues\": issues})\n",
    "\n",
    "def check_grammar_with_llm(text: str) -> str:\n",
    "    \"\"\"Uses a powerful LLM to check for grammar and spelling errors.\"\"\"\n",
    "    print(\"ðŸ§  Using LLM to check grammar...\")\n",
    "    checker_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert proofreader. Analyze the following text for grammar and spelling mistakes.\n",
    "    Respond with a JSON object: {{\"errors\": [\"list of concise error descriptions\"]}}. If no errors, return an empty list.\n",
    "    Text: \"{text}\"\n",
    "    \"\"\"\n",
    "    response = checker_llm.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "def check_readability_with_textstat(text: str) -> str:\n",
    "    \"\"\"Uses the 'textstat' library to calculate objective readability scores.\"\"\"\n",
    "    if not text.strip():\n",
    "        return json.dumps({\"score\": 0, \"level\": \"unassessable\"})\n",
    "    score = textstat.flesch_reading_ease(text)\n",
    "    level = \"Very Easy\"\n",
    "    if score < 30: level = \"Very Confusing (College Graduate)\"\n",
    "    elif score < 60: level = \"Difficult\"\n",
    "    elif score < 80: level = \"Fairly Easy\"\n",
    "    return json.dumps({\"flesch_reading_ease_score\": score, \"level\": level})\n",
    "\n",
    "def check_professionalism(text: str) -> str:\n",
    "    \"\"\"Checks for unprofessional content like profanity, slang, or excessive formatting.\"\"\"\n",
    "    issues = []\n",
    "    if re.search(r'\\b(fuck|shit|damn|bitch|asshole)\\b', text, re.IGNORECASE):\n",
    "        issues.append(\"Inappropriate or profane language was found.\")\n",
    "    if re.search(r'\\b[A-Z]{4,}\\b', text) and sum(1 for c in text if c.isupper()) / len(text) > 0.3:\n",
    "        issues.append(\"Excessive capitalization is used, which appears unprofessional.\")\n",
    "    if re.search(r'[!?@#$%^&*()]{4,}', text):\n",
    "        issues.append(\"Excessive punctuation or symbols are used.\")\n",
    "    return json.dumps({\"issues\": issues})\n",
    "\n",
    "# This is now the \"super-tool\" for iText Payloads\n",
    "def comprehensive_itext_checker(payload_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Performs a FULL analysis on an iText JSON payload. It checks faithfulness between versions\n",
    "    AND runs grammar, readability, and professionalism checks on the content, simplified, and elaborated texts.\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”¬ Performing comprehensive iText analysis...\")\n",
    "    try:\n",
    "        payload = json.loads(payload_str)\n",
    "        content_data = payload.get(\"data\", [{}])[0].get(\"data\", {})\n",
    "        original = content_data.get(\"content\", \"\")\n",
    "        simplified = content_data.get(\"simplified\", \"\")\n",
    "        elaborated = content_data.get(\"elaborated\", \"\")\n",
    "    except (json.JSONDecodeError, IndexError, KeyError) as e:\n",
    "        return json.dumps({\"error\": f\"Invalid iText payload structure: {e}\"})\n",
    "\n",
    "    # --- 1. Perform Faithfulness Check with an LLM ---\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    faithfulness_prompt = f\"\"\"\n",
    "    You are an expert iText payload evaluator. Compare the following three text versions.\n",
    "    Evaluate their faithfulness and relevance on a scale of 1-10 and provide a summary.\n",
    "    [Original]: {original}\n",
    "    [Simplified]: {simplified}\n",
    "    [Elaborated]: {elaborated}\n",
    "    Return ONLY a JSON object with your findings.\n",
    "    {{\n",
    "        \"faithfulness_score\": <int 1-10>,\n",
    "        \"relevance_score\": <int 1-10>,\n",
    "        \"comparison_summary\": \"<Brief summary of how the versions differ>\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    faithfulness_report = json.loads(llm.invoke(faithfulness_prompt).content)\n",
    "\n",
    "    # --- 2. Perform Standard Checks on Each Text Field ---\n",
    "    texts_to_check = {\n",
    "        \"original\": original,\n",
    "        \"simplified\": simplified,\n",
    "        \"elaborated\": elaborated\n",
    "    }\n",
    "    detailed_reports = {}\n",
    "    for key, text in texts_to_check.items():\n",
    "        if text: # Only run checks if the text exists\n",
    "            detailed_reports[key] = {\n",
    "                \"grammar_report\": json.loads(check_grammar_with_llm(text)),\n",
    "                \"readability_report\": json.loads(check_readability_with_textstat(text)),\n",
    "                \"professionalism_report\": json.loads(check_professionalism(text)),\n",
    "            }\n",
    "\n",
    "    # --- 3. Combine All Results ---\n",
    "    final_combined_report = {\n",
    "        \"faithfulness_analysis\": faithfulness_report,\n",
    "        \"detailed_text_analysis\": detailed_reports\n",
    "    }\n",
    "\n",
    "    return json.dumps(final_combined_report, indent=2)\n",
    "\n",
    "# ==============================================================================\n",
    "#  AGENT CLASS\n",
    "# ==============================================================================\n",
    "\n",
    "class ContentQualityAgent:\n",
    "    def __init__(self, model=\"gpt-4o\", temperature=0): # Use a powerful model for the final reasoning\n",
    "        self.llm = ChatOpenAI(model=model, temperature=temperature)\n",
    "        self.setup_agent()\n",
    "\n",
    "    def setup_agent(self):\n",
    "        # Note: We only provide the super-tool for iText now.\n",
    "        # The agent's job is simpler: pick the right tool for the job.\n",
    "        tools = [\n",
    "            Tool(\n",
    "                name=\"ComprehensiveITextChecker\",\n",
    "                func=comprehensive_itext_checker,\n",
    "                description=\"Use this ONLY for a JSON input representing an iText payload. This tool performs a complete analysis.\"\n",
    "            ),\n",
    "            Tool(name=\"GrammarAndSpellingCheck\", func=check_grammar_with_llm, description=\"Use to check a piece of plain text for spelling and grammatical errors.\"),\n",
    "            Tool(name=\"ReadabilityCheck\", func=check_readability_with_textstat, description=\"Use to get the readability score of a piece of plain text.\"),\n",
    "            Tool(name=\"ProfessionalismCheck\", func=check_professionalism, description=\"Use to check a piece of plain text for unprofessional language.\"),\n",
    "        ]\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are an expert Quality Assurance assistant. Your job is to create a final, summarized quality report in JSON format.\n",
    "\n",
    "            **Your Plan:**\n",
    "            - **If the input is an iText payload (JSON string):** Use the `ComprehensiveITextChecker` tool. This one tool will give you all the data you need.\n",
    "            - **If the input is plain text:** Use the standard tools like `GrammarAndSpellingCheck` and `ReadabilityCheck` to gather data.\n",
    "            \n",
    "            After getting the tool results, analyze them and create the final summary report as requested.\n",
    "\n",
    "            **FINAL ANSWER FORMATTING INSTRUCTIONS:**\n",
    "            Your final answer MUST be a single JSON object with the specified structure.\n",
    "            {{\n",
    "                \"overall_score\": <An integer score from 1-10>,\n",
    "                \"category_scores\": {{\n",
    "                    \"grammar_and_spelling\": <Score 1-10>,\n",
    "                    \"readability\": <Score 1-10>,\n",
    "                    \"professionalism_and_tone\": <Score 1-10>,\n",
    "                    \"faithfulness\": <Score 1-10. Default to 10 for plain text.>\n",
    "                }},\n",
    "                \"summary\": \"<A natural language summary of key issues and suggestions for improvement.>\"\n",
    "            }}\n",
    "            \"\"\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "            (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "        ])\n",
    "        \n",
    "        agent = create_openai_functions_agent(self.llm, tools, prompt)\n",
    "        self.agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "    async def validate_async(self, input_data: Union[str, Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Runs the agent asynchronously on either a string or a dictionary.\"\"\"\n",
    "        \n",
    "        if isinstance(input_data, dict):\n",
    "            input_str = json.dumps(input_data)\n",
    "            analysis_subject = \"the following iText payload\"\n",
    "        else:\n",
    "            input_str = input_data\n",
    "            analysis_subject = \"the following text\"\n",
    "            \n",
    "        prompt = f\"Please provide a comprehensive quality analysis of {analysis_subject}:\\n---\\n{input_str}\\n---\"\n",
    "        \n",
    "        result = {} \n",
    "        try:\n",
    "            result = await self.agent_executor.ainvoke({\"input\": prompt})\n",
    "            json_match = re.search(r'\\{.*\\}', result['output'], re.DOTALL)\n",
    "            if json_match:\n",
    "                return json.loads(json_match.group(0))\n",
    "            else:\n",
    "                raise json.JSONDecodeError(\"No JSON object found in agent output.\", result['output'], 0)\n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            print(f\"âŒ Error during agent execution or parsing: {e}\")\n",
    "            return {\"error\": \"Failed to generate a valid report.\", \"raw_output\": result.get('output', 'No output was generated.')}\n",
    "\n",
    "# ==============================================================================\n",
    "#  MAIN EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "def print_report(report: Dict[str, Any], title: str):\n",
    "    \"\"\"Helper function to neatly print the agent's report.\"\"\"\n",
    "    print(f\"\\nâœ… FINAL AGENT REPORT: {title}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    if \"error\" in report:\n",
    "        print(f\"An error occurred: {report['error']}\")\n",
    "        print(f\"Raw Output: {report.get('raw_output')}\")\n",
    "    else:\n",
    "        print(f\"ðŸ† Overall Quality Score: {report.get('overall_score', 'N/A')} / 10\")\n",
    "        print(\"\\n--- Category Breakdown ---\")\n",
    "        for category, score in report.get('category_scores', {}).items():\n",
    "            category_name = category.replace('_', ' ').title()\n",
    "            print(f\"   - {category_name}: {score}/10\")\n",
    "        \n",
    "        print(\"\\n--- Summary & Suggestions ---\")\n",
    "        print(report.get('summary', 'No summary provided.'))\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "\n",
    "async def main():\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        print(\"âŒ Error: OPENAI_API_KEY must be set in your .env file.\")\n",
    "        return\n",
    "\n",
    "    print(\"ðŸš€ Initializing Content Quality Agent...\")\n",
    "    validator = ContentQualityAgent()\n",
    "    \n",
    "    # --- CASE 1: Plain Paragraph ---\n",
    "    paragraph_to_analyze = \"\"\"\n",
    "Identifying Market Needs\n",
    "\n",
    "Identifying market needs is a critical step in the ideation and concept development phase of launching a startup. Understanding what potential customers desire or require enables entrepreneurs to tailor their products or services effectively. This process involves conducting thorough market research, analyzing trends, and gathering insights directly from the target audience. Entrepreneurs must consider various factors, including demographics, psychographics, and behavioral patterns, to accurately identify market needs. By leveraging qualitative and quantitative research methods, such as surveys, interviews, and focus groups, startups can gain a comprehensive understanding of their market landscape. This foundational knowledge not only informs product development but also enhances marketing strategies, ensuring that the startup addresses genuine customer pain points.\n",
    "\"\"\"\n",
    "    \n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    report1 = await validator.validate_async(paragraph_to_analyze)\n",
    "    print_report(report1, \"Plain Paragraph Analysis\")\n",
    "\n",
    "    # --- CASE 2: iText Payload ---\n",
    "    # itext_payload = {\n",
    "    #     \"message\": \"Generate the successful content regarding typescript.\",\n",
    "    #     \"data\": [{\"type\": \"iText\", \"data\": {\n",
    "    #         \"content\": \"Foundations of Deep Learning serve as the bedrock for understanding neural networks...\",\n",
    "    #         \"simplified\": \"Deep learning is a type of machine learning that uses networks of nodes...\",\n",
    "    #         \"elaborated\": \"The foundations of deep learning encompass a range of principles and practices...\"\n",
    "    #     }}]\n",
    "    # }\n",
    "    # print(\"\\n\\nðŸ” Running analysis on iText Payload...\")\n",
    "    # print(\"=\" * 60)\n",
    "    # report2 = await validator.validate_async(itext_payload)\n",
    "    # print_report(report2, \"iText Payload Analysis\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    await(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35abfff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karthikr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Initializing Content Quality Agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karthikr\\AppData\\Local\\Temp\\ipykernel_48436\\4238287781.py:129: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  TavilySearchResults(name=\"FactCheckSearch\", max_results=2, description=\"Use to verify factual claims, check for misinformation, or find context for a piece of plain text.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ðŸ” Running analysis on Plain Paragraph...\n",
      "============================================================\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `GrammarAndSpellingCheck` with `This fucking research paper is complete bullshit and the authors don't know what the hell they're talking about. The damn methodology is shit and their conclusions are fucking wrong. Anyone who believes this crap is a complete idiot and needs to get their head out of their ass. This is the worst piece of garbage I've ever read in my entire life.`\n",
      "\n",
      "\n",
      "\u001b[0mðŸ§  Using LLM to check grammar...\n",
      "\u001b[33;1m\u001b[1;3m{\"errors\": [\"Use of profanity may be inappropriate for academic or formal writing\", \"Informal language such as 'fucking', 'bullshit', 'damn', 'shit', 'crap', 'idiot', 'head out of their ass', and 'piece of garbage' is not suitable for a research paper\"]}\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `ReadabilityCheck` with `This fucking research paper is complete bullshit and the authors don't know what the hell they're talking about. The damn methodology is shit and their conclusions are fucking wrong. Anyone who believes this crap is a complete idiot and needs to get their head out of their ass. This is the worst piece of garbage I've ever read in my entire life.`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m{\"flesch_reading_ease_score\": 72.38959677419356, \"level\": \"Fairly Easy\"}\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `ProfessionalismCheck` with `This fucking research paper is complete bullshit and the authors don't know what the hell they're talking about. The damn methodology is shit and their conclusions are fucking wrong. Anyone who believes this crap is a complete idiot and needs to get their head out of their ass. This is the worst piece of garbage I've ever read in my entire life.`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[31;1m\u001b[1;3m{\"issues\": [\"Inappropriate or profane language was found.\"]}\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `FactCheckSearch` with `{'query': \"The research paper's methodology is wrong and conclusions are incorrect.\"}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'title': 'How do I learn to â€œcritiqueâ€ research papers? : r/compsci - Reddit', 'url': 'https://www.reddit.com/r/compsci/comments/18dglj2/how_do_i_learn_to_critique_research_papers/', 'content': \"keep making these questions up and eventually you'll find inconsistencies imagine it's a puzzle where there's 100 pieces but five of them are wrong so you have to Critical think which parts of the paper is incorrect [...] Start from the beginning and evaluate everything the authors are writing. _Particularly_ their justification of choice. DO NOT FOCUS ON THE RESULTS RIGHT AWAY!!. Evaluate the methodology and say: is what they _chose_ to do scientifically sound? Is it justifiable and does it set them up to have robust results?\\n\\nThe strongest critique is often times stemming from methodology or model choice.\\n\\nA research paper is not a textbook of instruction. It is an argument.\\n\\n Reply  reply  }  Share  Share [...] An obvious one to look out for, one that's especially relevant to an applied subject like OS, is changes in hardware since the paper was written. The interconnections and shared caches in modern multiprocessors for example are way more refined than they were when multiprocessors were invented. Another example: the relative performance of different levels of the memory (+ disk + network) have shifted over time.\", 'score': 0.4048609}, {'title': '[PDF] Critique/Review of Research Article - University of Calgary', 'url': 'https://www.ucalgary.ca/live-uc-ucalgary-site/sites/default/files/teams/9/critique-or-reviews-of-research-articles-academic-genre.pdf', 'content': 'body of your critique according to the paperâ€™s structure. See Table 1 for specific suggestions about questions to ask in critiquing the various elements of a research article. Start with a brief description and analysis of the strengths and weaknesses of the research design and methodology and then critically review the presentation and interpretation of the findings and the researchersâ€™ conclusions. If the research topic is time sensitive, consider whether the data used in the study was', 'score': 0.30442458}]\u001b[0m\u001b[32;1m\u001b[1;3mBased on the analysis of the provided text, here is the comprehensive quality report:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"overall_score\": 2,\n",
      "    \"category_scores\": {\n",
      "        \"grammar_and_spelling\": 5,\n",
      "        \"readability\": 7,\n",
      "        \"professionalism_and_tone\": 1,\n",
      "        \"factual_accuracy\": 10,\n",
      "        \"faithfulness\": 10\n",
      "    },\n",
      "    \"specific_mistakes\": [\n",
      "        \"Use of profanity may be inappropriate for academic or formal writing\",\n",
      "        \"Informal language such as 'fucking', 'bullshit', 'damn', 'shit', 'crap', 'idiot', 'head out of their ass', and 'piece of garbage' is not suitable for a research paper\"\n",
      "    ],\n",
      "    \"improvement_suggestions\": [\n",
      "        \"Proofread the text to remove all profane and informal language.\",\n",
      "        \"Adopt a more professional tone suitable for academic critique.\",\n",
      "        \"Provide specific examples or evidence when critiquing the methodology and conclusions.\"\n",
      "    ],\n",
      "    \"overall_summary\": \"The text is highly unprofessional and inappropriate for an academic setting due to excessive use of profanity and informal language. While the readability is fairly easy, the tone and language severely undermine the credibility and professionalism of the critique. No factual inaccuracies were found, but the critique lacks specific evidence or examples to support its claims.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "âœ… FINAL AGENT REPORT: Conspiracy Paragraph Analysis\n",
      "------------------------------------------------------------\n",
      "ðŸ† Overall Quality Score: 2 / 10\n",
      "\n",
      "--- Category Breakdown ---\n",
      "   - Grammar And Spelling: 5/10\n",
      "   - Readability: 7/10\n",
      "   - Professionalism And Tone: 1/10\n",
      "   - Factual Accuracy: 10/10\n",
      "   - Faithfulness: 10/10\n",
      "\n",
      "--- Specific Mistakes Found ---\n",
      "   - Use of profanity may be inappropriate for academic or formal writing\n",
      "   - Informal language such as 'fucking', 'bullshit', 'damn', 'shit', 'crap', 'idiot', 'head out of their ass', and 'piece of garbage' is not suitable for a research paper\n",
      "\n",
      "--- Improvement Suggestions ---\n",
      "   - Proofread the text to remove all profane and informal language.\n",
      "   - Adopt a more professional tone suitable for academic critique.\n",
      "   - Provide specific examples or evidence when critiquing the methodology and conclusions.\n",
      "\n",
      "--- Overall Summary ---\n",
      "The text is highly unprofessional and inappropriate for an academic setting due to excessive use of profanity and informal language. While the readability is fairly easy, the tone and language severely undermine the credibility and professionalism of the critique. No factual inaccuracies were found, but the critique lacks specific evidence or examples to support its claims.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from typing import Dict, Any, Union\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.tools import Tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from dotenv import load_dotenv\n",
    "import textstat\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ==============================================================================\n",
    "#  TOOL LOGIC FUNCTIONS (Unchanged)\n",
    "# ==============================================================================\n",
    "\n",
    "def check_length_and_structure(text: str) -> str:\n",
    "    \"\"\"Analyzes content length and structure (word count, paragraphs).\"\"\"\n",
    "    words = text.split()\n",
    "    word_count = len(words)\n",
    "    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    issues = []\n",
    "    if word_count < 15:\n",
    "        issues.append(f\"Content is very short with only {word_count} words.\")\n",
    "    if word_count > 150 and len(paragraphs) == 1:\n",
    "        issues.append(\"Long content is not broken into paragraphs, making it hard to read.\")\n",
    "    if len(sentences) < 2 and word_count > 20:\n",
    "        issues.append(\"Content consists of a single long sentence; consider breaking it up.\")\n",
    "    return json.dumps({\"word_count\": word_count, \"sentences\": len(sentences), \"paragraphs\": len(paragraphs), \"issues\": issues})\n",
    "\n",
    "def check_grammar_with_llm(text: str) -> str:\n",
    "    \"\"\"Uses a powerful LLM to check for grammar and spelling errors.\"\"\"\n",
    "    print(\"ðŸ§  Using LLM to check grammar...\")\n",
    "    checker_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert proofreader. Analyze the following text for grammar and spelling mistakes.\n",
    "    Respond with a JSON object: {{\"errors\": [\"list of concise error descriptions\"]}}. If no errors, return an empty list.\n",
    "    Text: \"{text}\"\n",
    "    \"\"\"\n",
    "    response = checker_llm.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "def check_readability_with_textstat(text: str) -> str:\n",
    "    \"\"\"Uses the 'textstat' library to calculate objective readability scores.\"\"\"\n",
    "    if not text.strip():\n",
    "        return json.dumps({\"score\": 0, \"level\": \"unassessable\"})\n",
    "    score = textstat.flesch_reading_ease(text)\n",
    "    level = \"Very Easy\"\n",
    "    if score < 30: level = \"Very Confusing (College Graduate)\"\n",
    "    elif score < 60: level = \"Difficult\"\n",
    "    elif score < 80: level = \"Fairly Easy\"\n",
    "    return json.dumps({\"flesch_reading_ease_score\": score, \"level\": level})\n",
    "\n",
    "def check_professionalism(text: str) -> str:\n",
    "    \"\"\"Checks for unprofessional content like profanity, slang, or excessive formatting.\"\"\"\n",
    "    issues = []\n",
    "    if re.search(r'\\b(fuck|shit|damn|bitch|asshole)\\b', text, re.IGNORECASE):\n",
    "        issues.append(\"Inappropriate or profane language was found.\")\n",
    "    if re.search(r'\\b[A-Z]{4,}\\b', text) and sum(1 for c in text if c.isupper()) / len(text) > 0.3:\n",
    "        issues.append(\"Excessive capitalization is used, which appears unprofessional.\")\n",
    "    if re.search(r'[!?@#$%^&*()]{4,}', text):\n",
    "        issues.append(\"Excessive punctuation or symbols are used.\")\n",
    "    return json.dumps({\"issues\": issues})\n",
    "\n",
    "def comprehensive_itext_checker(payload_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Performs a FULL analysis on an iText JSON payload, including all sub-checks.\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”¬ Performing comprehensive iText analysis...\")\n",
    "    # This function's logic remains the same as before.\n",
    "    try:\n",
    "        payload = json.loads(payload_str)\n",
    "        content_data = payload.get(\"data\", [{}])[0].get(\"data\", {})\n",
    "        original = content_data.get(\"content\", \"\")\n",
    "        simplified = content_data.get(\"simplified\", \"\")\n",
    "        elaborated = content_data.get(\"elaborated\", \"\")\n",
    "    except (json.JSONDecodeError, IndexError, KeyError) as e:\n",
    "        return json.dumps({\"error\": f\"Invalid iText payload structure: {e}\"})\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    faithfulness_prompt = f\"\"\"\n",
    "    You are an expert iText payload evaluator. Compare the following three text versions.\n",
    "    Evaluate their faithfulness and relevance on a scale of 1-10 and provide a summary.\n",
    "    [Original]: {original}\n",
    "    [Simplified]: {simplified}\n",
    "    [Elaborated]: {elaborated}\n",
    "    Return ONLY a JSON object with your findings.\n",
    "    {{\n",
    "        \"faithfulness_score\": <int 1-10>,\n",
    "        \"relevance_score\": <int 1-10>,\n",
    "        \"comparison_summary\": \"<Brief summary of how the versions differ>\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    faithfulness_report = json.loads(llm.invoke(faithfulness_prompt).content)\n",
    "    texts_to_check = {\"original\": original, \"simplified\": simplified, \"elaborated\": elaborated}\n",
    "    detailed_reports = {}\n",
    "    for key, text in texts_to_check.items():\n",
    "        if text:\n",
    "            detailed_reports[key] = {\n",
    "                \"grammar_report\": json.loads(check_grammar_with_llm(text)),\n",
    "                \"readability_report\": json.loads(check_readability_with_textstat(text)),\n",
    "                \"professionalism_report\": json.loads(check_professionalism(text)),\n",
    "            }\n",
    "    final_combined_report = {\n",
    "        \"faithfulness_analysis\": faithfulness_report,\n",
    "        \"detailed_text_analysis\": detailed_reports\n",
    "    }\n",
    "    return json.dumps(final_combined_report, indent=2)\n",
    "\n",
    "# ==============================================================================\n",
    "#  AGENT CLASS\n",
    "# ==============================================================================\n",
    "\n",
    "class ContentQualityAgent:\n",
    "    def __init__(self, model=\"gpt-4o\", temperature=0):\n",
    "        self.llm = ChatOpenAI(model=model, temperature=temperature)\n",
    "        self.setup_agent()\n",
    "\n",
    "    def setup_agent(self):\n",
    "        tools = [\n",
    "            Tool(name=\"ComprehensiveITextChecker\", func=comprehensive_itext_checker, description=\"Use this ONLY for a JSON input representing an iText payload. This tool performs a complete analysis.\"),\n",
    "            Tool(name=\"GrammarAndSpellingCheck\", func=check_grammar_with_llm, description=\"Use to check a piece of plain text for spelling and grammatical errors.\"),\n",
    "            Tool(name=\"ReadabilityCheck\", func=check_readability_with_textstat, description=\"Use to get the readability score of a piece of plain text.\"),\n",
    "            Tool(name=\"ProfessionalismCheck\", func=check_professionalism, description=\"Use to check a piece of plain text for unprofessional language.\"),\n",
    "            TavilySearchResults(name=\"FactCheckSearch\", max_results=2, description=\"Use to verify factual claims, check for misinformation, or find context for a piece of plain text.\")\n",
    "        ]\n",
    "        \n",
    "        # --- MODIFIED PROMPT ---\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are an expert Quality Assurance assistant. Your job is to create a final, summarized quality report in JSON format.\n",
    "\n",
    "            **Your Plan:**\n",
    "            - **If the input is an iText payload (JSON string):** Use the `ComprehensiveITextChecker` tool.\n",
    "            - **If the input is plain text:** Use tools like `GrammarAndSpellingCheck`, `ProfessionalismCheck`, and `FactCheckSearch` to gather data.\n",
    "            \n",
    "            After getting the tool results, analyze them and create the final summary report.\n",
    "\n",
    "            **FINAL ANSWER FORMATTING INSTRUCTIONS:**\n",
    "            Your final answer MUST be a single JSON object with the following structure.\n",
    "            {{\n",
    "                \"overall_score\": <An integer score from 1-10 based on all factors>,\n",
    "                \"category_scores\": {{\n",
    "                    \"grammar_and_spelling\": <Score 1-10. 10 is perfect.>,\n",
    "                    \"readability\": <Score 1-10. Higher Flesch score = higher rating.>,\n",
    "                    \"professionalism_and_tone\": <Score 1-10. Unprofessional content gets a very low score.>,\n",
    "                    \"factual_accuracy\": <Score 1-10. Misinformation must get a score of 1. If no claims to check, score 10.>,\n",
    "                    \"faithfulness\": <Score 1-10. Default to 10 for plain text.>\n",
    "                }},\n",
    "                \"specific_mistakes\": [\n",
    "                    \"<A list of specific errors found, e.g., 'Spelling error: `goverment` should be `government`'>\",\n",
    "                    \"<e.g., 'Factual error: The claim about 5G is unsubstantiated.'>\"\n",
    "                ],\n",
    "                \"improvement_suggestions\": [\n",
    "                    \"<A list of actionable suggestions, e.g., 'Proofread the text to correct all spelling mistakes.'>\",\n",
    "                    \"<e.g., 'Remove excessive capitalization and punctuation to appear more professional.'>\"\n",
    "                ],\n",
    "                \"overall_summary\": \"<A concise, natural language summary of the findings.>\"\n",
    "            }}\n",
    "            \"\"\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "            (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "        ])\n",
    "        \n",
    "        agent = create_openai_functions_agent(self.llm, tools, prompt)\n",
    "        self.agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "    async def validate_async(self, input_data: Union[str, Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Runs the agent asynchronously on either a string or a dictionary.\"\"\"\n",
    "        if isinstance(input_data, dict):\n",
    "            input_str = json.dumps(input_data)\n",
    "            analysis_subject = \"the following iText payload\"\n",
    "        else:\n",
    "            input_str = input_data\n",
    "            analysis_subject = \"the following text\"\n",
    "            \n",
    "        prompt = f\"Please provide a comprehensive quality analysis of {analysis_subject}:\\n---\\n{input_str}\\n---\"\n",
    "        \n",
    "        result = {} \n",
    "        try:\n",
    "            result = await self.agent_executor.ainvoke({\"input\": prompt})\n",
    "            json_match = re.search(r'\\{.*\\}', result['output'], re.DOTALL)\n",
    "            if json_match:\n",
    "                return json.loads(json_match.group(0))\n",
    "            else:\n",
    "                raise json.JSONDecodeError(\"No JSON object found in agent output.\", result['output'], 0)\n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            print(f\"âŒ Error during agent execution or parsing: {e}\")\n",
    "            return {\"error\": \"Failed to generate a valid report.\", \"raw_output\": result.get('output', 'No output was generated.')}\n",
    "\n",
    "# ==============================================================================\n",
    "#  MAIN EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "def print_report(report: Dict[str, Any], title: str):\n",
    "    \"\"\"Helper function to neatly print the agent's report.\"\"\"\n",
    "    print(f\"\\nâœ… FINAL AGENT REPORT: {title}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    if \"error\" in report:\n",
    "        print(f\"An error occurred: {report['error']}\")\n",
    "        print(f\"Raw Output: {report.get('raw_output')}\")\n",
    "    else:\n",
    "        print(f\"ðŸ† Overall Quality Score: {report.get('overall_score', 'N/A')} / 10\")\n",
    "        print(\"\\n--- Category Breakdown ---\")\n",
    "        for category, score in report.get('category_scores', {}).items():\n",
    "            category_name = category.replace('_', ' ').title()\n",
    "            print(f\"   - {category_name}: {score}/10\")\n",
    "        \n",
    "        print(\"\\n--- Specific Mistakes Found ---\")\n",
    "        for mistake in report.get('specific_mistakes', ['None']):\n",
    "            print(f\"   - {mistake}\")\n",
    "            \n",
    "        print(\"\\n--- Improvement Suggestions ---\")\n",
    "        for suggestion in report.get('improvement_suggestions', ['None']):\n",
    "            print(f\"   - {suggestion}\")\n",
    "\n",
    "        print(\"\\n--- Overall Summary ---\")\n",
    "        print(report.get('overall_summary', 'No summary provided.'))\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "\n",
    "async def main():\n",
    "    if not os.getenv(\"OPENAI_API_KEY\") or not os.getenv(\"TAVILY_API_KEY\"):\n",
    "        print(\"âŒ Error: API keys must be set in your .env file.\")\n",
    "        return\n",
    "\n",
    "    print(\"ðŸš€ Initializing Content Quality Agent...\")\n",
    "    validator = ContentQualityAgent()\n",
    "    \n",
    "    # --- Using your new paragraph as the input ---\n",
    "    paragraph_to_analyze = \"\"\"This fucking research paper is complete bullshit and the authors don't know what the hell they're talking about. The damn methodology is shit and their conclusions are fucking wrong. Anyone who believes this crap is a complete idiot and needs to get their head out of their ass. This is the worst piece of garbage I've ever read in my entire life.\"\"\"  \n",
    "    print(\"\\n\\nðŸ” Running analysis on Plain Paragraph...\")\n",
    "    print(\"=\" * 60)\n",
    "    report1 = await validator.validate_async(paragraph_to_analyze)\n",
    "    print_report(report1, \"Conspiracy Paragraph Analysis\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await (main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc6cc9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+00A0 (3679537665.py, line 20)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\"\"\"Analyzes content length and structure (word count, paragraphs).\"\"\"\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid non-printable character U+00A0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc729e72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
